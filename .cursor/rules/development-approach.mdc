---
alwaysApply: true
---

# Development Approach - Test-Driven & Metrics-Based

## Development Philosophy
We follow an incremental, metrics-driven approach:

1. **Start Basic** - Implement core functionality first
2. **Test Everything** - Unit tests before adding complexity
3. **Measure Performance** - Collect P95, latency, throughput metrics
4. **Iterate Based on Data** - Add complexity only when metrics justify it

## Core API Endpoints (Priority Order)

### 1. Essential Endpoints
- `POST /fit/{series_id}` - Training API (primary functionality)
- `POST /predict/{series_id}` - Inference API (performance critical)
- `GET /healthcheck` - Service health monitoring
- `GET /plot?series_id={sensor_XYZ}&version={version_string}` - Visualization for debugging/validation

### 2. Testing Strategy
- **Unit Tests First** - Test each component in isolation
- **Integration Tests** - Test API endpoints with real data
- **Performance Tests** - Measure against requirements (180 RPS, P95 < 100ms)
- **Load Tests** - Validate under realistic traffic

### 3. Metrics Collection
Always measure:
- **Latency**: P50, P95, P99 response times
- **Throughput**: Requests per second capacity
- **Resource Usage**: CPU, memory, database connections
- **Error Rates**: Success/failure ratios

### 4. Complexity Gates
Only add complexity (caching, load balancing, async queues) after:
- Core functionality proven with tests
- Performance bottlenecks identified with metrics
- Clear improvement targets established

## Current Focus
Start with basic implementation, comprehensive testing, then optimize based on measured performance gaps.