# ðŸ§ª Test Suite

Estrutura de testes organizados e reutilizÃ¡veis para todos os serviÃ§os.

## ðŸ“‚ Estrutura

```
tests/
â”œâ”€â”€ conftest.py              # Fixtures globais e configuraÃ§Ã£o
â”œâ”€â”€ pytest.ini              # ConfiguraÃ§Ã£o do pytest
â”œâ”€â”€ requirements.txt         # DependÃªncias de teste
â”œâ”€â”€ unit/                    # Testes unitÃ¡rios
â”‚   â”œâ”€â”€ test_training_service.py
â”‚   â”œâ”€â”€ test_inference_service.py
â”‚   â””â”€â”€ test_plot_service.py
â”œâ”€â”€ integration/             # Testes de integraÃ§Ã£o
â”‚   â””â”€â”€ test_end_to_end.py
â””â”€â”€ utils/                   # UtilitÃ¡rios reutilizÃ¡veis
    â””â”€â”€ test_helpers.py
```

## ðŸš€ ExecuÃ§Ã£o dos Testes

### Testes unitÃ¡rios (funcionando)
```bash
python scripts/run_unit_tests_only.py
# ou manualmente:
pytest tests/unit/test_ml_algorithm.py tests/unit/test_services_simplified.py -v
```

### Todos os testes (com alguns problemas de setup)
```bash
python scripts/run_tests.py
```

### Apenas testes unitÃ¡rios (todos, incluindo com problemas)
```bash
python scripts/run_tests.py unit
# ou
pytest tests/unit/ -v
```

### Apenas testes de integraÃ§Ã£o
```bash
python scripts/run_tests.py integration  
# ou
pytest tests/integration/ -v
```

### Testes especÃ­ficos
```bash
# Teste especÃ­fico
pytest tests/unit/test_training_service.py::TestTrainingService::test_fit_model_success -v

# Por marcadores
pytest -m "not integration" -v  # Apenas nÃ£o-integraÃ§Ã£o
pytest -m "database" -v         # Apenas testes que usam BD
```

## ðŸ—ï¸ Fixtures ReutilizÃ¡veis

### `conftest.py`
- `test_db`: SessÃ£o de banco de dados de teste
- `mock_redis`: Mock do Redis
- `sample_training_data`: Dados de treinamento de exemplo
- `sample_prediction_data`: Dados de prediÃ§Ã£o de exemplo
- `trained_model_in_db`: Modelo jÃ¡ treinado no banco

### `test_helpers.py`
- `DataGenerator`: Gera dados de teste (normal, com anomalias, constante)
- `ServiceTester`: Helpers para testar serviÃ§os
- `TestValidator`: ValidaÃ§Ã£o de formatos de resposta

## ðŸ§ª Tipos de Teste

### 1. Testes UnitÃ¡rios
Testam cada serviÃ§o isoladamente com mocks:

```python
def test_fit_model_success(self, training_client, sample_training_data):
    response = training_client.post("/fit/test_series", json=sample_training_data)
    assert response.status_code == 200
```

### 2. Testes de IntegraÃ§Ã£o
Testam o fluxo completo entre serviÃ§os:

```python
def test_complete_workflow(self):
    # 1. Train model
    train_response = requests.post(f"{training_url}/fit/{series_id}", json=data)
    
    # 2. Make prediction  
    predict_response = requests.post(f"{inference_url}/predict/{series_id}", json=pred_data)
    
    # 3. Get plot
    plot_response = requests.get(f"{plot_url}/plot?series_id={series_id}")
```

## ðŸ“Š CenÃ¡rios de Teste

### Training Service
- âœ… Treinamento bem-sucedido
- âœ… Dados invÃ¡lidos (tamanhos diferentes, poucos pontos)
- âœ… Valores constantes (falha esperada)
- âœ… CriaÃ§Ã£o de registros no banco
- âœ… Health check

### Inference Service  
- âœ… PrediÃ§Ã£o com modelo em cache
- âœ… Cache miss (modelo nÃ£o encontrado)
- âœ… Dados invÃ¡lidos
- âœ… DetecÃ§Ã£o de anomalias
- âœ… Cache de resultados
- âœ… Health check com Redis

### Plot Service
- âœ… RecuperaÃ§Ã£o de dados de treinamento
- âœ… VersÃ£o especÃ­fica vs. mais recente
- âœ… SÃ©ries inexistentes
- âœ… Estrutura correta dos dados
- âœ… Health check

### IntegraÃ§Ã£o End-to-End
- âœ… Fluxo completo: train â†’ predict â†’ plot
- âœ… ConsistÃªncia de versÃµes
- âœ… Tratamento de erros
- âœ… Health checks de todos os serviÃ§os
- âœ… PrediÃ§Ãµes concorrentes

## ðŸ”§ ConfiguraÃ§Ã£o

### PrÃ©-requisitos
```bash
# Instalar dependÃªncias de teste
pip install -r tests/requirements.txt

# Para testes de integraÃ§Ã£o, serviÃ§os devem estar rodando:
make start  # ou docker-compose up
```

### VariÃ¡veis de Ambiente
```bash
export ENVIRONMENT=test
export DATABASE_URL=sqlite:///./test.db
export REDIS_HOST=localhost
export REDIS_PORT=6379
```

## ðŸ“ˆ MÃ©tricas de Teste

### Coverage
```bash
pytest --cov=services/ --cov-report=html tests/
```

### Performance
```bash
pytest --benchmark-only tests/
```

### RelatÃ³rios
```bash
pytest --html=reports/report.html --self-contained-html tests/
```

## ðŸ› Debug

### Logs detalhados
```bash
pytest -v -s --log-cli-level=DEBUG tests/
```

### Parar no primeiro erro
```bash
pytest -x tests/
```

### Rodar teste especÃ­fico em debug
```bash
pytest tests/unit/test_training_service.py::TestTrainingService::test_fit_model_success -v -s --pdb
```

## âœ… Checklist de Testes

Antes de fazer deploy:

- [ ] Todos os testes unitÃ¡rios passam
- [ ] Todos os testes de integraÃ§Ã£o passam  
- [ ] Coverage > 80%
- [ ] Performance dentro dos requisitos (P95 < 100ms)
- [ ] Health checks funcionando
- [ ] Tratamento de erros validado
